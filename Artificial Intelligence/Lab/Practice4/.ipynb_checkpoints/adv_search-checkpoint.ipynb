{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import TicTacToe\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "infinity = math.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting Started with the Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . .\n",
      ". . .\n",
      ". . .\n",
      "\n",
      "{(0, 1), (1, 2), (2, 1), (0, 0), (1, 1), (2, 0), (0, 2), (2, 2), (1, 0)}\n",
      ". . .\n",
      "X . .\n",
      ". . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initializing a TicTacToe game\n",
    "game = TicTacToe(height=3, width=3, k=3)\n",
    "board = game.initial\n",
    "# get the possible actions\n",
    "actions = game.actions(board)\n",
    "# to apply a move (the board automatically figures out which player)\n",
    "board_after_move = game.result(board, list(actions)[0])\n",
    "\n",
    "print(board)\n",
    "print(actions)\n",
    "print(board_after_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game_step(game, state, strategies: dict, verbose=False):\n",
    "    start = time.perf_counter()\n",
    "    player = state.to_move\n",
    "    move = strategies[player](game, state)\n",
    "    state = game.result(state, move)\n",
    "    time_elapsed = time.perf_counter() - start\n",
    "    if verbose: \n",
    "        print('Player', player, 'move:', move, f'time: {time_elapsed:.4f}s', )\n",
    "        print(state)\n",
    "    return state\n",
    "\n",
    "def play_game(game, strategies: dict, verbose=False):\n",
    "    \"\"\"Play a turn-taking game. `strategies` is a {player_name: function} dict,\n",
    "    where function(state, game) is used to get the player's move.\"\"\"\n",
    "    state = game.initial\n",
    "    while not game.is_terminal(state):\n",
    "        state = play_game_step(game, state, strategies, verbose)\n",
    "    return state\n",
    "\n",
    "# setup a random strategy for testing\n",
    "def random_player(game, state): return random.choice(list(game.actions(state)))\n",
    "\n",
    "def search_player(search_algorithm):\n",
    "    \"\"\"A game player who uses the specified search algorithm\"\"\"\n",
    "    return lambda game, state: search_algorithm(game, state)[1] # we expect our search algorithm to return (v, move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a game\n",
    "game = TicTacToe()\n",
    "state = game.initial\n",
    "strategies = dict(X=random_player, O=random_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player X move: (2, 1) time: 0.0001s\n",
      ". . .\n",
      ". . X\n",
      ". . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# option 1: \n",
    "# play_game(game, strategies, True)\n",
    "\n",
    "# option 2: run it step by step by executing this cell multiple times until the game end\n",
    "if not game.is_terminal(state):\n",
    "    state = play_game_step(game, state, strategies, True)\n",
    "else:\n",
    "    print(f\"Game result: {game.utility(state, 'X')}\")\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MINIMAX and Alpha-Beta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MINIMAX-DECISION and EXPECTIMINIMAX\n",
    "\n",
    "__function__ MINIMAX-DECISION(_state_) __returns__ _an action_  \n",
    "&emsp;__return__ arg max<sub> _a_ &Element; ACTIONS(_s_)</sub> MIN\\-VALUE(RESULT(_state_, _a_))  \n",
    "\n",
    "---\n",
    "__function__ MAX\\-VALUE(_state_) __returns__ _a utility value_  \n",
    "&emsp;__if__ TERMINAL\\-TEST(_state_) __then return__ UTILITY(_state_)  \n",
    "&emsp;_v_ &larr; &minus;&infin;  \n",
    "&emsp;__for each__ _a_ __in__ ACTIONS(_state_) __do__  \n",
    "&emsp;&emsp;&emsp;_v_ &larr; MAX(_v_, MIN\\-VALUE(RESULT(_state_, _a_)))  \n",
    "&emsp;__return__ _v_  \n",
    "\n",
    "---\n",
    "__function__ MIN\\-VALUE(_state_) __returns__ _a utility value_  \n",
    "&emsp;__if__ TERMINAL\\-TEST(_state_) __then return__ UTILITY(_state_)  \n",
    "&emsp;_v_ &larr; &infin;  \n",
    "&emsp;__for each__ _a_ __in__ ACTIONS(_state_) __do__  \n",
    "&emsp;&emsp;&emsp;_v_ &larr; MIN(_v_, MAX\\-VALUE(RESULT(_state_, _a_)))  \n",
    "&emsp;__return__ _v_  \n",
    "\n",
    "---\n",
    "__function__ EXPECTIMINIMAX(_s_) =     \n",
    "&emsp;UTILITY(_s_) __if__ TERMINAL\\-TEST(_s_)  \n",
    "&emsp;max<sub>_a_</sub> EXPECTIMINIMAX(RESULT(_s, a_)) __if__ PLAYER(_s_)= MAX  \n",
    "&emsp;min<sub>_a_</sub> EXPECTIMINIMAX(RESULT(_s, a_)) __if__ PLAYER(_s_)= MIN  \n",
    "&emsp;âˆ‘<sub>_r_</sub> P(_r_) EXPECTIMINIMAX(RESULT(_s, r_)) __if__ PLAYER(_s_)= CHANCE  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c8dd56004660>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(game, strategies, verbose)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c8dd56004660>\u001b[0m in \u001b[0;36mplay_game_step\u001b[0;34m(game, state, strategies, verbose)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c8dd56004660>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(game, state)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msearch_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_algorithm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m\"\"\"A game player who uses the specified search algorithm\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msearch_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# we expect our search algorithm to return (v, move)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-e099d8be889d>\u001b[0m in \u001b[0;36mminimax_search\u001b[0;34m(game, state)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# test against random_player\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e099d8be889d>\u001b[0m in \u001b[0;36mmax_value\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0minfinity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;31m# TODO: decide *v* and *move*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e099d8be889d>\u001b[0m in \u001b[0;36mmin_value\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmin_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# TODO: implement function min_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'v' is not defined"
     ]
    }
   ],
   "source": [
    "def minimax_search(game, state):\n",
    "    \"\"\"Search game tree to determine best move; return (value, move) pair.\"\"\"\n",
    "\n",
    "    player = state.to_move\n",
    "\n",
    "    def max_value(state):\n",
    "        if game.is_terminal(state):\n",
    "            return game.utility(state, player), None\n",
    "        v, move = -infinity, None\n",
    "        for a in game.actions(state):\n",
    "            v2, _ = min_value(game.result(state, a))\n",
    "            # TODO: decide *v* and *move*\n",
    "            \n",
    "        return v, move\n",
    "\n",
    "    def min_value(state):\n",
    "        # TODO: implement function min_value\n",
    "        return v, move\n",
    "\n",
    "    return max_value(state)\n",
    "\n",
    "# test against random_player\n",
    "%time play_game(TicTacToe(), dict(X=search_player(minimax_search), O=random_player), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have implemented it right, it will always be a draw for h=w=k=3\n",
    "%time play_game(TicTacToe(), dict(X=search_player(minimax_search), O=search_player(minimax_search)), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALPHA-BETA-SEARCH\n",
    "\n",
    "__function__ ALPHA-BETA-SEARCH(_state_) __returns__ an action  \n",
    "&emsp;_v_ &larr; MAX\\-VALUE(_state_, &minus;&infin;, &plus;&infin;)  \n",
    "&emsp;__return__ the _action_ in ACTIONS(_state_) with value _v_  \n",
    "\n",
    "---\n",
    "__function__ MAX\\-VALUE(_state_, _&alpha;_, _&beta;_) __returns__ _a utility value_  \n",
    "&emsp;__if__ TERMINAL\\-TEST(_state_) __then return__ UTILITY(_state_)  \n",
    "&emsp;_v_ &larr; &minus;&infin;  \n",
    "&emsp;__for each__ _a_ __in__ ACTIONS(_state_) __do__  \n",
    "&emsp;&emsp;&emsp;_v_ &larr; MAX(_v_, MIN\\-VALUE(RESULT(_state_, _a_), _&alpha;_, _&beta;_))  \n",
    "&emsp;&emsp;&emsp;__if__ _v_ &ge; _&beta;_ __then return__ _v_  \n",
    "&emsp;&emsp;&emsp;_&alpha;_ &larr; MAX(_&alpha;_, _v_)  \n",
    "&emsp;__return__ _v_  \n",
    "\n",
    "---\n",
    "__function__ MIN\\-VALUE(_state_, _&alpha;_, _&beta;_) __returns__ _a utility value_  \n",
    "&emsp;__if__ TERMINAL\\-TEST(_state_) __then return__ UTILITY(_state_)  \n",
    "&emsp;_v_ &larr; &plus;&infin;  \n",
    "&emsp;__for each__ _a_ __in__ ACTIONS(_state_) __do__  \n",
    "&emsp;&emsp;&emsp;_v_ &larr; MIN(_v_, MAX\\-VALUE(RESULT(_state_, _a_), _&alpha;_, _&beta;_))  \n",
    "&emsp;&emsp;&emsp;__if__ _v_ &le; _&alpha;_ __then return__ _v_  \n",
    "&emsp;&emsp;&emsp;_&beta;_ &larr; MIN(_&beta;_, _v_)  \n",
    "&emsp;__return__ _v_  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabeta_search(game, state):\n",
    "    \"\"\"Search game to determine best action; use alpha-beta pruning.\n",
    "    As in [Figure 5.7], this version searches all the way to the leaves.\"\"\"\n",
    "\n",
    "    player = state.to_move\n",
    "\n",
    "    def max_value(state, alpha, beta):\n",
    "        if game.is_terminal(state):\n",
    "            return game.utility(state, player), None\n",
    "        v, move = -infinity, None\n",
    "        for a in game.actions(state):\n",
    "            v2, _ = min_value(game.result(state, a), alpha, beta)\n",
    "            if v2 > v:\n",
    "                v, move = v2, a\n",
    "                alpha = max(alpha, v)\n",
    "            if v >= beta:\n",
    "                return v, move\n",
    "        return v, move\n",
    "\n",
    "    def min_value(state, alpha, beta):\n",
    "        # TODO: implement function min_value\n",
    "            \n",
    "        return v, move\n",
    "\n",
    "    return max_value(state, -infinity, +infinity)\n",
    "\n",
    "# test against random_player\n",
    "%time play_game(TicTacToe(), dict(X=search_player(alphabeta_search), O=random_player), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time play_game(TicTacToe(), dict(X=search_player(alphabeta_search), O=search_player(alphabeta_search)), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Heuristic Cutoffs and Monte-Carlo Tree Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff_depth(d):\n",
    "    \"\"\"A cutoff function that searches to depth d.\"\"\"\n",
    "    return lambda game, state, depth: depth > d\n",
    "\n",
    "def h_alphabeta_search(cutoff=cutoff_depth(6), h=lambda s, p: 0):\n",
    "    def search(game, state):\n",
    "        \"\"\"\n",
    "        Search game to determine best action; use alpha-beta pruning.\n",
    "        As in [Figure 5.7], this version searches all the way to the leaves.\n",
    "        TODO: add checks for *cutoff* and return *h* at proper locations\n",
    "        \"\"\"\n",
    "\n",
    "        player = state.to_move\n",
    "\n",
    "        def max_value(state, alpha, beta, depth):\n",
    "            if game.is_terminal(state):\n",
    "                return game.utility(state, player), None\n",
    "\n",
    "            v, move = -infinity, None\n",
    "            for a in game.actions(state):\n",
    "                v2, _ = min_value(game.result(state, a), alpha, beta, depth+1)\n",
    "                if v2 > v:\n",
    "                    v, move = v2, a\n",
    "                    alpha = max(alpha, v)\n",
    "                if v >= beta:\n",
    "                    return v, move\n",
    "            return v, move\n",
    "\n",
    "        def min_value(state, alpha, beta, depth):\n",
    "            if game.is_terminal(state):\n",
    "                return game.utility(state, player), None\n",
    "\n",
    "            v, move = +infinity, None\n",
    "            for a in game.actions(state):\n",
    "                v2, _ = max_value(game.result(state, a), alpha, beta, depth + 1)\n",
    "                if v2 < v:\n",
    "                    v, move = v2, a\n",
    "                    beta = min(beta, v)\n",
    "                if v <= alpha:\n",
    "                    return v, move\n",
    "            return v, move\n",
    "\n",
    "        return max_value(state, -infinity, +infinity, 0)\n",
    "    return search\n",
    "\n",
    "# plug in your own *h* and see its performance against a random player\n",
    "game = TicTacToe()\n",
    "\n",
    "# I'll just cheat with the true utility for a demo (won't work when the tree is deeper)\n",
    "h = lambda s, p: game.utility(s, p) \n",
    "cutoff = cutoff_depth(9)\n",
    "\n",
    "%time play_game(game, {'X':search_player(h_alphabeta_search(cutoff=cutoff, h=h)), 'O':random_player}, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTE-CARLO-TREE-SEARCH\n",
    "\n",
    "__function__ MONTE-CARLO-TREE-SEARCH(_state_) __returns__ an action  \n",
    "&emsp;tree &larr; NODE(_state_)\n",
    "&emsp;__while__ TIME\\-REMAINING() __do__  \n",
    "&emsp;&emsp;&emsp;__tree__ &larr; PLAYOUT(_tree_)  \n",
    "&emsp;__return__ the _move_ in ACTIONS(_state_) with highest Q(_state_,_move_)  \n",
    "\n",
    "---\n",
    "\n",
    "__function__ PLAYOUT(_tree_) __returns__ _updated tree_  \n",
    "&emsp;_node_ &larr; _tree_  \n",
    "&emsp;__while__ _node_ is not terminal and was already in _tree_ __do__  \n",
    "&emsp;&emsp;&emsp;_move_ &larr; SELECT(_node_)  \n",
    "&emsp;&emsp;&emsp;_node_ &larr; FOLLOW\\-LINK(_node_,_move_)  \n",
    "&emsp;_outcome_ &larr; SIMULATION(_node_.STATE)  \n",
    "&emsp;UPDATE(_node_,_outcome_)  \n",
    "&emsp;__return__ _tree_  \n",
    "\n",
    "---\n",
    "\n",
    "__function__ SELECT(_node_) __returns__ _an action_  \n",
    "&emsp;__return__ argmax<sub>m &isin; FEASIBLE\\-ACTIONS(_node_)</sub> UCB(RESULT(_node_,_m_))  \n",
    "\n",
    "---\n",
    "\n",
    "__function__ UCB(_child_) __returns__ _a number_  \n",
    "&emsp;__return__ _child_.VALUE + C &times; <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\sqrt{\\frac{\\log{child.PARENT.N}}{child.N}}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/png.latex?\\inline&space;\\sqrt{\\frac{\\log{child.PARENT.N}}{child.N}}\" title=\"\\sqrt{\\frac{\\log{child.PARENT.N}}{child.N}}\" /></a> \n",
    "\n",
    "---\n",
    "\n",
    "The Monte Carlo tree search algorithm. A game tree, _tree_, is initialized, and then grows by one node with each call to PLAYOUT. The function SELECT chooses a move that best balances exploitation and exploration according to the UCB formula. FOLLOW-LINK traverses from the current node by making a move; this could be to a previously-seen node, or to a new node that is added to the tree. Once we have added a new node, we exit the __while__ loop and SIMULATION chooses moves (with a randomized policy that is designed to favor good moves but to compute quickly) until the game is over. Then, UPDATE updates all the nodes in the tree from node to the root, recording the fact that the path led to the final __outcome__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class MCT_Node:\n",
    "    \"\"\"Node in the Monte Carlo search tree, keeps track of the children states.\"\"\"\n",
    "\n",
    "    def __init__(self, parent=None, state=None, U=0, N=0):\n",
    "        self.__dict__.update(parent=parent, state=state, U=U, N=N)\n",
    "        self.children = {} # dict of {node: action} or {actione: node}\n",
    "        self.actions = None\n",
    "\n",
    "def ucb(n, C=1.4):\n",
    "    return np.inf if n.N == 0 else n.U / n.N + C * np.sqrt(np.log(n.parent.N) / n.N)\n",
    "\n",
    "def monte_carlo_tree_search(game, state, N=1000):\n",
    "    def select(n):\n",
    "        \"\"\"\n",
    "        select a leaf node in the tree according to their UCB values\n",
    "        \"\"\"\n",
    "        if n.children:\n",
    "            return select(max(n.children.keys(), key=ucb))\n",
    "        else:\n",
    "            return n\n",
    "\n",
    "    def expand(n):\n",
    "        \"\"\" \n",
    "        expand the leaf node by adding all its children states (FOLLOW-LINK)\n",
    "        \"\"\"\n",
    "        if not n.children and not game.is_terminal(n.state):\n",
    "            n.children = {None: None for action in game.actions(n.state)} # TODO: fix this\n",
    "        return select(n)\n",
    "\n",
    "    def simulate(game, state):\n",
    "        \"\"\" \n",
    "        simulate the utility of current state using a random strategy\n",
    "        \"\"\"\n",
    "        player = state.to_move\n",
    "        while not game.is_terminal(state):\n",
    "            # TODO: fix and complete this\n",
    "            state = state\n",
    "            \n",
    "        v = 0\n",
    "        return v\n",
    "\n",
    "    def backprop(n, utility):\n",
    "        \"\"\"\n",
    "        passing the utility back to all parent nodes\n",
    "        \"\"\"\n",
    "        if utility > 0:\n",
    "            n.U += utility\n",
    "        n.N += 1\n",
    "        if n.parent:\n",
    "            backprop(n.parent, -utility)\n",
    "\n",
    "    root = MCT_Node(state=state)\n",
    "\n",
    "    for _ in range(N):\n",
    "        # PLAYOUT\n",
    "        leaf = select(root)\n",
    "        child = expand(leaf)\n",
    "        result = simulate(game, child.state)\n",
    "        backprop(child, result)\n",
    "    \n",
    "    # TODO: select the action\n",
    "    v, move = 0, (0, 0)\n",
    "    return v, move\n",
    "\n",
    "# if you have done it right it should usually win\n",
    "%time play_game(game, {'X':search_player(monte_carlo_tree_search), 'O':random_player}, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Further Hints on Project\n",
    "\n",
    "* Be **absolutely sure** that you are not maximizing your opponent's utility!\n",
    "* Code optimization: the performance of a python program greatly depends on its implementation. Write your code to be as efficient as possible and maybe search for performance hacks!\n",
    "* Performance tuning: gather statistics to help yourself in deciding the hyperparameters, e.g.:\n",
    "  * +\n",
    "  * ..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5708518ee693b55567314c7d79d1222d0ee40b55ee536a3aec67969691c5a596"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
